# -*- coding: utf-8 -*-
"""Version_5.3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Jsiz_4-A2935Ihi4IcDJc2QsywcACipv
"""

import yfinance as yf
import pandas as pd

selected_stocks = ["BAJFINANCE.NS", "HDFCAMC.NS", "ASIANPAINT.NS", "TCS.NS", "DRREDDY.NS"]

# Updated duration of data: 10 years
start_date = "2014-01-01"
end_date = "2024-01-01"

# Dataframe to store the collected data
stock_data = {}

# Fetching data for each selected stock
for stock in selected_stocks:
    stock_data[stock] = yf.download(stock, start=start_date, end=end_date)

import numpy as np

for stock, df in stock_data.items():
    # Checking for missing values and filling them
    df.fillna(method='ffill', inplace=True)  # forward fill for missing values



    # Calculating log returns
    df['log_return'] = np.log(df['Adj Close'] / df['Adj Close'].shift(1))

def calculate_RSI(data, window=14):
    """ Calculate Relative Strength Index (RSI) """
    delta = data.diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
    rs = gain / loss
    rsi = 100 - (100 / (1 + rs))
    return rsi

def calculate_MACD(data, n_fast=12, n_slow=26, n_signal=9):
    """ Calculate Moving Average Convergence Divergence (MACD) """
    exp1 = data.ewm(span=n_fast, adjust=False).mean()
    exp2 = data.ewm(span=n_slow, adjust=False).mean()
    macd = exp1 - exp2
    signal = macd.ewm(span=n_signal, adjust=False).mean()
    return macd, signal, macd - signal

def calculate_BB(data, window=20, num_std_dev=2):
    """ Calculate Bollinger Bands """
    mean = data.rolling(window=window).mean()
    std_dev = data.rolling(window=window).std()
    upper_band = mean + (std_dev * num_std_dev)
    lower_band = mean - (std_dev * num_std_dev)
    return upper_band, mean, lower_band

# Apply calculations to each stock
for stock, df in stock_data.items():
    # RSI
    df['RSI'] = calculate_RSI(df['Adj Close'])

    # MACD
    df['MACD'], df['MACD_signal'], df['MACD_hist'] = calculate_MACD(df['Adj Close'])

    # Bollinger Bands
    df['Upper_BB'], df['Middle_BB'], df['Lower_BB'] = calculate_BB(df['Adj Close'])

def calculate_fibonacci_retracement(data):
    """ Calculate Fibonacci Retracement levels """
    max_price = data.max()
    min_price = data.min()
    difference = max_price - min_price
    first_level = max_price - difference * 0.236
    second_level = max_price - difference * 0.382
    third_level = max_price - difference * 0.5
    fourth_level = max_price - difference * 0.618
    return first_level, second_level, third_level, fourth_level

for stock, df in stock_data.items():

    levels = calculate_fibonacci_retracement(df['Adj Close'])
    df['Fibonacci_Level_1'], df['Fibonacci_Level_2'], df['Fibonacci_Level_3'], df['Fibonacci_Level_4'] = levels

# Fetch Nifty 50 data
nifty_data = yf.download("^NSEI", start=start_date, end=end_date)

# Calculating a simple moving average for Nifty 50 as a trend indicator
nifty_data['Nifty_50_SMA'] = nifty_data['Adj Close'].rolling(window=50).mean()

# Merging Nifty trend data with each stock data
for stock, df in stock_data.items():
    df = df.join(nifty_data['Nifty_50_SMA'], on='Date', how='left')

import requests
from bs4 import BeautifulSoup
from textblob import TextBlob
import pandas as pd

# Define the stocks and their corresponding Yahoo News URLs
stocks_urls = {
    "BAJFINANCE.NS": "https://news.yahoo.com/stock/BAJFINANCE.NS",
    "HDFCAMC.NS": "https://news.yahoo.com/stock/HDFCAMC.NS",
    "ASIANPAINT.NS": "https://news.yahoo.com/stock/ASIANPAINT.NS",
    "TCS.NS": "https://news.yahoo.com/stock/TCS.NS",
    "DRREDDY.NS": "https://news.yahoo.com/stock/DRREDDY.NS"
}

# Function to scrape news headlines
def scrape_headlines(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    headlines = soup.find_all('h3')  # Assuming headlines are in <h3> tags
    return [headline.get_text() for headline in headlines]

# Function to analyze sentiment of headlines
def analyze_sentiment(headlines):
    sentiment_scores = []
    for headline in headlines:
        analysis = TextBlob(headline)
        sentiment_scores.append(analysis.sentiment.polarity)
    return sentiment_scores

# Adding Nifty 50 SMA to each stock's DataFrame
for stock, df in stock_data.items():
    df['Nifty_50_SMA'] = nifty_data['Adj Close'].rolling(window=50).mean()

# Ensure all relevant features including the newly added ones are present
X = combined_data[['RSI', 'MACD', 'Upper_BB', 'Lower_BB', 'Fibonacci_Level_1', 'Nifty_50_SMA']]

# Combining all stock data into a single DataFrame
combined_data = pd.concat(stock_data.values())

# Main process
stock_sentiments = {}
for stock, url in stocks_urls.items():
    headlines = scrape_headlines(url)
    sentiments = analyze_sentiment(headlines)
    stock_sentiments[stock] = pd.DataFrame({
        'Headline': headlines,
        'Sentiment': sentiments
    })

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import xgboost as xgb
from keras.models import Sequential
from keras.layers import LSTM, Dense

import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder

# Combining stock data and market trend data
all_stock_data = pd.DataFrame()
for stock, df in stock_data.items():
    df['Stock'] = stock  # Add a column to identify the stock
    all_stock_data = pd.concat([all_stock_data, df])

# Calculate weekly returns and identify best performers
all_stock_data['Week_Number'] = all_stock_data.index.week
weekly_best_performers = all_stock_data.groupby('Week_Number').apply(lambda x: x['Adj Close'].pct_change().idxmax())

import numpy as np

# Assuming each DataFrame in stock_data has necessary columns for calculations
for stock, df in stock_data.items():
    # Example calculations
    df['Volatility'] = df['Adj Close'].pct_change().rolling(window=30).std() * np.sqrt(252)  # Annualized volatility
    df['Weekly_Return'] = df['Adj Close'].pct_change(periods=5)
    df['Positive_Indicators'] = (df['RSI'] > 50).astype(int) + (df['MACD'] > df['MACD_signal']).astype(int)  # etc.

    # Risk-Reward Ratio (Sharpe Ratio, adjust as needed)
    df['Risk_Reward'] = df['Weekly_Return'] / df['Volatility']

    # Composite Score ( adjust weights as needed)
    df['Composite_Score'] = 0.4 * df['Weekly_Return'] + 0.3 * df['Risk_Reward'] + 0.3 * df['Positive_Indicators']
    df['Week_Number'] = df.index.isocalendar().week

# Combine all individual stock DataFrames
combined_data = pd.concat(stock_data.values())

# Identify the best performer each week based on the highest composite score
combined_data['Best_Performer'] = combined_data.groupby('Week_Number')['Composite_Score'].transform(lambda x: x == x.max())
combined_data['Best_Performer'] = combined_data['Best_Performer'].astype(int)

# Your target variable
y = combined_data['Best_Performer']

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Imputing NaN values with the mean of the column
imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X)

# Now, scale the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imputed)

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, shuffle=False)

# Reshaping data for LSTM
X_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
import xgboost as xgb
from keras.models import Sequential
from keras.layers import LSTM, Dense

# Gradient Boosting
gb_model = GradientBoostingClassifier()
gb_model.fit(X_train, y_train)

# Random Forest
rf_model = RandomForestClassifier()
rf_model.fit(X_train, y_train)

# XGBoost
xgb_model = xgb.XGBClassifier()
xgb_model.fit(X_train, y_train)

# LSTM
lstm_model = Sequential()
lstm_model.add(LSTM(50, activation='relu', input_shape=(1, X_train.shape[1])))
lstm_model.add(Dense(1, activation='sigmoid'))
lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
lstm_model.fit(X_train_reshaped, y_train, epochs=100, batch_size=32)

from sklearn.metrics import classification_report

# Evaluate GB, RF, XGB
for model in [gb_model, rf_model, xgb_model]:
    predictions = model.predict(X_test)
    print(model.__class__.__name__)
    print(classification_report(y_test, predictions))

# Evaluate LSTM
lstm_predictions = lstm_model.predict(X_test_reshaped)
lstm_predictions = (lstm_predictions > 0.5).astype(int)  # Assuming binary classification
print("LSTM Model")
print(classification_report(y_test, lstm_predictions))

from imblearn.over_sampling import SMOTE

# Resample the training data
smote = SMOTE()
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Gradient Boosting with resampled data
gb_model.fit(X_train_resampled, y_train_resampled)

# Random Forest with class weights
rf_model = RandomForestClassifier(class_weight='balanced')
rf_model.fit(X_train_resampled, y_train_resampled)

# XGBoost with scale_pos_weight parameter
scale_pos_weight = sum(y_train == 0) / sum(y_train == 1)
xgb_model = xgb.XGBClassifier(scale_pos_weight=scale_pos_weight)
xgb_model.fit(X_train_resampled, y_train_resampled)

# Evaluate XGBoost
xgb_predictions = xgb_model.predict(X_test)
print("XGBoost Classifier Evaluation")
print(classification_report(y_test, xgb_predictions))

from sklearn.metrics import precision_recall_curve

# Get probability estimates for class 1
probabilities = gb_model.predict_proba(X_test)[:, 1]

# Calculate precision-recall pairs for different threshold values
precisions, recalls, thresholds = precision_recall_curve(y_test, probabilities)

# Select a threshold that balances precision and recall according to your needs
# This is an illustrative example; choose a threshold based on your specific requirements
selected_threshold = thresholds[np.argmax(precisions >= 0.05)]  # Example threshold criteria
adjusted_predictions = (probabilities >= selected_threshold).astype(int)

# Evaluate with the new threshold
print("Adjusted Gradient Boosting Classifier Evaluation")
print(classification_report(y_test, adjusted_predictions))

from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout, Bidirectional
from sklearn.model_selection import train_test_split
import numpy as np

# Data preparation as before
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, shuffle=False)
X_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))

# Building an advanced LSTM model
lstm_model = Sequential()
lstm_model.add(Bidirectional(LSTM(50, return_sequences=True, input_shape=(1, X_train.shape[1]))))
lstm_model.add(Dropout(0.2))
lstm_model.add(LSTM(50, return_sequences=False))
lstm_model.add(Dropout(0.2))
lstm_model.add(Dense(1, activation='sigmoid'))

lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Training the model
lstm_model.fit(X_train_reshaped, y_train, epochs=100, batch_size=32)

from sklearn.metrics import classification_report, accuracy_score

# Predicting with the LSTM model
lstm_predictions = lstm_model.predict(X_test_reshaped)

# Since we're doing binary classification,  might want to convert probabilities to binary predictions
# can adjust the threshold based on y specific needs (default is 0.5)
lstm_predictions_binary = (lstm_predictions > 0.5).astype(int)

# Evaluating the model
print("LSTM Model Performance:")
print(classification_report(y_test, lstm_predictions_binary))
print("Accuracy:", accuracy_score(y_test, lstm_predictions_binary))

from sklearn.utils import class_weight

# Calculate class weights
class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
class_weight_dict = dict(enumerate(class_weights))

# Use these class weights in model.fit
lstm_model.fit(X_train_reshaped, y_train, epochs=100, batch_size=32, class_weight=class_weight_dict)

from sklearn.metrics import classification_report, accuracy_score

# Predicting with the LSTM model
lstm_predictions = lstm_model.predict(X_test_reshaped)
lstm_predictions_binary = (lstm_predictions > 0.5).astype(int)  # Convert probabilities to binary predictions

# Evaluating the model
print("LSTM Model Performance with Class Weights:")
print(classification_report(y_test, lstm_predictions_binary))
print("Accuracy:", accuracy_score(y_test, lstm_predictions_binary))

from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout, Bidirectional

# Enhanced LSTM Model
lstm_model = Sequential()
lstm_model.add(Bidirectional(LSTM(50, return_sequences=True, input_shape=(1, X_train.shape[1]))))
lstm_model.add(Dropout(0.3))
lstm_model.add(LSTM(100, return_sequences=False))
lstm_model.add(Dropout(0.3))
lstm_model.add(Dense(50, activation='relu'))
lstm_model.add(Dense(1, activation='sigmoid'))

lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Assuming class_weight_dict is already calculated
lstm_model.fit(X_train_reshaped, y_train, epochs=100, batch_size=32, class_weight=class_weight_dict)

from sklearn.metrics import classification_report, accuracy_score
import numpy as np

# Predicting probabilities
lstm_probabilities = lstm_model.predict(X_test_reshaped)

# Adjusting the decision threshold
threshold = 0.5
lstm_predictions = (lstm_probabilities > threshold).astype(int)

# Evaluating the model
print("Adjusted LSTM Model Performance:")
print(classification_report(y_test, lstm_predictions))
print("Accuracy:", accuracy_score(y_test, lstm_predictions))

# Simplified LSTM Model
lstm_model = Sequential()
lstm_model.add(LSTM(50, input_shape=(1, X_train.shape[1])))
lstm_model.add(Dropout(0.2))
lstm_model.add(Dense(1, activation='sigmoid'))

lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Retraining the model with class weights
lstm_model.fit(X_train_reshaped, y_train, epochs=100, batch_size=32, class_weight=class_weight_dict)

# Predicting probabilities
lstm_probabilities = lstm_model.predict(X_test_reshaped)

# Experiment with different thresholds
thresholds = [0.4, 0.5, 0.6]  # Example thresholds, adjust as needed

for threshold in thresholds:
    lstm_predictions = (lstm_probabilities > threshold).astype(int)
    print(f"LSTM Model Performance at Threshold {threshold}:")
    print(classification_report(y_test, lstm_predictions))
    print("Accuracy:", accuracy_score(y_test, lstm_predictions))
    print("-------------------------------------------")