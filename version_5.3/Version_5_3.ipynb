{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GDcAly3A3jNZ"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selected_stocks = [\"BAJFINANCE.NS\", \"HDFCAMC.NS\", \"ASIANPAINT.NS\", \"TCS.NS\", \"DRREDDY.NS\"]\n",
        "\n"
      ],
      "metadata": {
        "id": "TqhGgrhc4AYo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated duration of data: 10 years\n",
        "start_date = \"2014-01-01\"\n",
        "end_date = \"2024-01-01\"\n",
        "\n",
        "# Dataframe to store the collected data\n",
        "stock_data = {}"
      ],
      "metadata": {
        "id": "e9fp4PpR6UST"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetching data for each selected stock\n",
        "for stock in selected_stocks:\n",
        "    stock_data[stock] = yf.download(stock, start=start_date, end=end_date)"
      ],
      "metadata": {
        "id": "hPYlngYV6VWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "nLEyy_Q76_TY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for stock, df in stock_data.items():\n",
        "    # Checking for missing values and filling them\n",
        "    df.fillna(method='ffill', inplace=True)  # forward fill for missing values\n",
        "\n",
        "\n",
        "\n",
        "    # Calculating log returns\n",
        "    df['log_return'] = np.log(df['Adj Close'] / df['Adj Close'].shift(1))"
      ],
      "metadata": {
        "id": "bggeBPrt7ARO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_RSI(data, window=14):\n",
        "    \"\"\" Calculate Relative Strength Index (RSI) \"\"\"\n",
        "    delta = data.diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
        "    rs = gain / loss\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi"
      ],
      "metadata": {
        "id": "R9kB4tAt7HEq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_MACD(data, n_fast=12, n_slow=26, n_signal=9):\n",
        "    \"\"\" Calculate Moving Average Convergence Divergence (MACD) \"\"\"\n",
        "    exp1 = data.ewm(span=n_fast, adjust=False).mean()\n",
        "    exp2 = data.ewm(span=n_slow, adjust=False).mean()\n",
        "    macd = exp1 - exp2\n",
        "    signal = macd.ewm(span=n_signal, adjust=False).mean()\n",
        "    return macd, signal, macd - signal"
      ],
      "metadata": {
        "id": "wqIA1B3S70HQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_BB(data, window=20, num_std_dev=2):\n",
        "    \"\"\" Calculate Bollinger Bands \"\"\"\n",
        "    mean = data.rolling(window=window).mean()\n",
        "    std_dev = data.rolling(window=window).std()\n",
        "    upper_band = mean + (std_dev * num_std_dev)\n",
        "    lower_band = mean - (std_dev * num_std_dev)\n",
        "    return upper_band, mean, lower_band"
      ],
      "metadata": {
        "id": "bA33uzJz738t"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply calculations to each stock\n",
        "for stock, df in stock_data.items():\n",
        "    # RSI\n",
        "    df['RSI'] = calculate_RSI(df['Adj Close'])\n",
        "\n",
        "    # MACD\n",
        "    df['MACD'], df['MACD_signal'], df['MACD_hist'] = calculate_MACD(df['Adj Close'])\n",
        "\n",
        "    # Bollinger Bands\n",
        "    df['Upper_BB'], df['Middle_BB'], df['Lower_BB'] = calculate_BB(df['Adj Close'])"
      ],
      "metadata": {
        "id": "-y0ij4ev8Qwl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_fibonacci_retracement(data):\n",
        "    \"\"\" Calculate Fibonacci Retracement levels \"\"\"\n",
        "    max_price = data.max()\n",
        "    min_price = data.min()\n",
        "    difference = max_price - min_price\n",
        "    first_level = max_price - difference * 0.236\n",
        "    second_level = max_price - difference * 0.382\n",
        "    third_level = max_price - difference * 0.5\n",
        "    fourth_level = max_price - difference * 0.618\n",
        "    return first_level, second_level, third_level, fourth_level"
      ],
      "metadata": {
        "id": "CazYiKfm8SYj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for stock, df in stock_data.items():\n",
        "\n",
        "    levels = calculate_fibonacci_retracement(df['Adj Close'])\n",
        "    df['Fibonacci_Level_1'], df['Fibonacci_Level_2'], df['Fibonacci_Level_3'], df['Fibonacci_Level_4'] = levels"
      ],
      "metadata": {
        "id": "FvwYCGtI8bNG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch Nifty 50 data\n",
        "nifty_data = yf.download(\"^NSEI\", start=start_date, end=end_date)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egTcZyY88dW5",
        "outputId": "dbe4f060-8cce-4b0b-ed9c-38b67ea9d812"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating a simple moving average for Nifty 50 as a trend indicator\n",
        "nifty_data['Nifty_50_SMA'] = nifty_data['Adj Close'].rolling(window=50).mean()"
      ],
      "metadata": {
        "id": "X7xYjelU8kch"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging Nifty trend data with each stock data\n",
        "for stock, df in stock_data.items():\n",
        "    df = df.join(nifty_data['Nifty_50_SMA'], on='Date', how='left')"
      ],
      "metadata": {
        "id": "Kpg1hAxg8qKe"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from textblob import TextBlob\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "9qGKYGKF-dOL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the stocks and their corresponding Yahoo News URLs\n",
        "stocks_urls = {\n",
        "    \"BAJFINANCE.NS\": \"https://news.yahoo.com/stock/BAJFINANCE.NS\",\n",
        "    \"HDFCAMC.NS\": \"https://news.yahoo.com/stock/HDFCAMC.NS\",\n",
        "    \"ASIANPAINT.NS\": \"https://news.yahoo.com/stock/ASIANPAINT.NS\",\n",
        "    \"TCS.NS\": \"https://news.yahoo.com/stock/TCS.NS\",\n",
        "    \"DRREDDY.NS\": \"https://news.yahoo.com/stock/DRREDDY.NS\"\n",
        "}"
      ],
      "metadata": {
        "id": "VYkDIZIS-gAv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to scrape news headlines\n",
        "def scrape_headlines(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    headlines = soup.find_all('h3')  # Assuming headlines are in <h3> tags\n",
        "    return [headline.get_text() for headline in headlines]"
      ],
      "metadata": {
        "id": "6BwSaqKD-jyE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to analyze sentiment of headlines\n",
        "def analyze_sentiment(headlines):\n",
        "    sentiment_scores = []\n",
        "    for headline in headlines:\n",
        "        analysis = TextBlob(headline)\n",
        "        sentiment_scores.append(analysis.sentiment.polarity)\n",
        "    return sentiment_scores"
      ],
      "metadata": {
        "id": "HpLKSuuo-kwI"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding Nifty 50 SMA to each stock's DataFrame\n",
        "for stock, df in stock_data.items():\n",
        "    df['Nifty_50_SMA'] = nifty_data['Adj Close'].rolling(window=50).mean()"
      ],
      "metadata": {
        "id": "CMCNtMN-CFCW"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure all relevant features including the newly added ones are present\n",
        "X = combined_data[['RSI', 'MACD', 'Upper_BB', 'Lower_BB', 'Fibonacci_Level_1', 'Nifty_50_SMA']]"
      ],
      "metadata": {
        "id": "o2KJ7cedCNYg"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining all stock data into a single DataFrame\n",
        "combined_data = pd.concat(stock_data.values())"
      ],
      "metadata": {
        "id": "Z_STtRm_CJ1j"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main process\n",
        "stock_sentiments = {}\n",
        "for stock, url in stocks_urls.items():\n",
        "    headlines = scrape_headlines(url)\n",
        "    sentiments = analyze_sentiment(headlines)\n",
        "    stock_sentiments[stock] = pd.DataFrame({\n",
        "        'Headline': headlines,\n",
        "        'Sentiment': sentiments\n",
        "    })"
      ],
      "metadata": {
        "id": "GE44v-Yd-qeo"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import xgboost as xgb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense"
      ],
      "metadata": {
        "id": "jB6dgKQD-s-G"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder"
      ],
      "metadata": {
        "id": "REdMobw3AXAP"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining stock data and market trend data\n",
        "all_stock_data = pd.DataFrame()\n",
        "for stock, df in stock_data.items():\n",
        "    df['Stock'] = stock  # Add a column to identify the stock\n",
        "    all_stock_data = pd.concat([all_stock_data, df])"
      ],
      "metadata": {
        "id": "ae3BPxTFAYIm"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate weekly returns and identify best performers\n",
        "all_stock_data['Week_Number'] = all_stock_data.index.week\n",
        "weekly_best_performers = all_stock_data.groupby('Week_Number').apply(lambda x: x['Adj Close'].pct_change().idxmax())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGSl-05CAbbu",
        "outputId": "ce702096-04e2-4f0a-d8a1-d3a1d0cd75ba"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-37383af5b577>:2: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series. To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
            "  all_stock_data['Week_Number'] = all_stock_data.index.week\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming each DataFrame in stock_data has necessary columns for calculations\n",
        "for stock, df in stock_data.items():\n",
        "    # Example calculations\n",
        "    df['Volatility'] = df['Adj Close'].pct_change().rolling(window=30).std() * np.sqrt(252)  # Annualized volatility\n",
        "    df['Weekly_Return'] = df['Adj Close'].pct_change(periods=5)\n",
        "    df['Positive_Indicators'] = (df['RSI'] > 50).astype(int) + (df['MACD'] > df['MACD_signal']).astype(int)  # etc.\n",
        "\n",
        "    # Risk-Reward Ratio (Sharpe Ratio, adjust as needed)\n",
        "    df['Risk_Reward'] = df['Weekly_Return'] / df['Volatility']\n",
        "\n",
        "    # Composite Score ( adjust weights as needed)\n",
        "    df['Composite_Score'] = 0.4 * df['Weekly_Return'] + 0.3 * df['Risk_Reward'] + 0.3 * df['Positive_Indicators']\n",
        "    df['Week_Number'] = df.index.isocalendar().week\n",
        "\n",
        "# Combine all individual stock DataFrames\n",
        "combined_data = pd.concat(stock_data.values())\n",
        "\n",
        "# Identify the best performer each week based on the highest composite score\n",
        "combined_data['Best_Performer'] = combined_data.groupby('Week_Number')['Composite_Score'].transform(lambda x: x == x.max())\n",
        "combined_data['Best_Performer'] = combined_data['Best_Performer'].astype(int)\n",
        "\n",
        "# Your target variable\n",
        "y = combined_data['Best_Performer']\n"
      ],
      "metadata": {
        "id": "r-mnaSVzC8NG"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Imputing NaN values with the mean of the column\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "# Now, scale the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_imputed)\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, shuffle=False)\n"
      ],
      "metadata": {
        "id": "gYeSnax-Edow"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshaping data for LSTM\n",
        "X_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n"
      ],
      "metadata": {
        "id": "mnYX1VclD7Jh"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "import xgboost as xgb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense"
      ],
      "metadata": {
        "id": "Ffcd4IVdD_kS"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Boosting\n",
        "gb_model = GradientBoostingClassifier()\n",
        "gb_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "-DjFDsshEDQd",
        "outputId": "8a9dc452-a9a1-4412-9600-c597014bed08"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingClassifier()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest\n",
        "rf_model = RandomForestClassifier()\n",
        "rf_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "eUxONu4dEqUS",
        "outputId": "0a9b4f9a-0388-4ea1-8b0e-babad4fc3d2c"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier()"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost\n",
        "xgb_model = xgb.XGBClassifier()\n",
        "xgb_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "k9oealR0E0NI",
        "outputId": "b9893932-6b31-4408-c7a6-f473c309e990"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "              gamma=None, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
              "              num_parallel_tree=None, random_state=None, ...)"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "              gamma=None, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
              "              num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "              gamma=None, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
              "              num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(LSTM(50, activation='relu', input_shape=(1, X_train.shape[1])))\n",
        "lstm_model.add(Dense(1, activation='sigmoid'))\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "lstm_model.fit(X_train_reshaped, y_train, epochs=100, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGpRnZDgE2EK",
        "outputId": "a1998ed8-af32-4720-a6b8-bea3a2182331"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "280/280 [==============================] - 5s 3ms/step - loss: 0.3215 - accuracy: 0.9817\n",
            "Epoch 2/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0519 - accuracy: 0.9952\n",
            "Epoch 3/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0353 - accuracy: 0.9952\n",
            "Epoch 4/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0312 - accuracy: 0.9952\n",
            "Epoch 5/100\n",
            "280/280 [==============================] - 1s 5ms/step - loss: 0.0291 - accuracy: 0.9952\n",
            "Epoch 6/100\n",
            "280/280 [==============================] - 1s 5ms/step - loss: 0.0277 - accuracy: 0.9952\n",
            "Epoch 7/100\n",
            "280/280 [==============================] - 1s 5ms/step - loss: 0.0268 - accuracy: 0.9952\n",
            "Epoch 8/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0261 - accuracy: 0.9952\n",
            "Epoch 9/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0257 - accuracy: 0.9952\n",
            "Epoch 10/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0254 - accuracy: 0.9952\n",
            "Epoch 11/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0252 - accuracy: 0.9952\n",
            "Epoch 12/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0251 - accuracy: 0.9952\n",
            "Epoch 13/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0250 - accuracy: 0.9952\n",
            "Epoch 14/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0249 - accuracy: 0.9952\n",
            "Epoch 15/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0248 - accuracy: 0.9952\n",
            "Epoch 16/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0246 - accuracy: 0.9952\n",
            "Epoch 17/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0246 - accuracy: 0.9952\n",
            "Epoch 18/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0246 - accuracy: 0.9952\n",
            "Epoch 19/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0245 - accuracy: 0.9952\n",
            "Epoch 20/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.0244 - accuracy: 0.9952\n",
            "Epoch 21/100\n",
            "280/280 [==============================] - 1s 5ms/step - loss: 0.0242 - accuracy: 0.9952\n",
            "Epoch 22/100\n",
            "280/280 [==============================] - 1s 5ms/step - loss: 0.0243 - accuracy: 0.9952\n",
            "Epoch 23/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0242 - accuracy: 0.9952\n",
            "Epoch 24/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0243 - accuracy: 0.9952\n",
            "Epoch 25/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0241 - accuracy: 0.9952\n",
            "Epoch 26/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0241 - accuracy: 0.9952\n",
            "Epoch 27/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0240 - accuracy: 0.9952\n",
            "Epoch 28/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0240 - accuracy: 0.9952\n",
            "Epoch 29/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0238 - accuracy: 0.9952\n",
            "Epoch 30/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0240 - accuracy: 0.9952\n",
            "Epoch 31/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0239 - accuracy: 0.9952\n",
            "Epoch 32/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0237 - accuracy: 0.9952\n",
            "Epoch 33/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0239 - accuracy: 0.9952\n",
            "Epoch 34/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0236 - accuracy: 0.9952\n",
            "Epoch 35/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0238 - accuracy: 0.9952\n",
            "Epoch 36/100\n",
            "280/280 [==============================] - 1s 5ms/step - loss: 0.0237 - accuracy: 0.9952\n",
            "Epoch 37/100\n",
            "280/280 [==============================] - 1s 5ms/step - loss: 0.0236 - accuracy: 0.9952\n",
            "Epoch 38/100\n",
            "280/280 [==============================] - 1s 5ms/step - loss: 0.0237 - accuracy: 0.9952\n",
            "Epoch 39/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0236 - accuracy: 0.9952\n",
            "Epoch 40/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0236 - accuracy: 0.9952\n",
            "Epoch 41/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0235 - accuracy: 0.9952\n",
            "Epoch 42/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0235 - accuracy: 0.9952\n",
            "Epoch 43/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0234 - accuracy: 0.9952\n",
            "Epoch 44/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0233 - accuracy: 0.9952\n",
            "Epoch 45/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0232 - accuracy: 0.9952\n",
            "Epoch 46/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0233 - accuracy: 0.9952\n",
            "Epoch 47/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0232 - accuracy: 0.9952\n",
            "Epoch 48/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0233 - accuracy: 0.9952\n",
            "Epoch 49/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0233 - accuracy: 0.9952\n",
            "Epoch 50/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0232 - accuracy: 0.9952\n",
            "Epoch 51/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.0230 - accuracy: 0.9952\n",
            "Epoch 52/100\n",
            "280/280 [==============================] - 1s 5ms/step - loss: 0.0232 - accuracy: 0.9952\n",
            "Epoch 53/100\n",
            "280/280 [==============================] - 1s 5ms/step - loss: 0.0230 - accuracy: 0.9952\n",
            "Epoch 54/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0230 - accuracy: 0.9952\n",
            "Epoch 55/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0229 - accuracy: 0.9952\n",
            "Epoch 56/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0230 - accuracy: 0.9952\n",
            "Epoch 57/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0228 - accuracy: 0.9952\n",
            "Epoch 58/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0229 - accuracy: 0.9952\n",
            "Epoch 59/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0229 - accuracy: 0.9952\n",
            "Epoch 60/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0229 - accuracy: 0.9952\n",
            "Epoch 61/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0227 - accuracy: 0.9952\n",
            "Epoch 62/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0228 - accuracy: 0.9952\n",
            "Epoch 63/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0227 - accuracy: 0.9952\n",
            "Epoch 64/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0228 - accuracy: 0.9952\n",
            "Epoch 65/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0226 - accuracy: 0.9952\n",
            "Epoch 66/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.0226 - accuracy: 0.9952\n",
            "Epoch 67/100\n",
            "280/280 [==============================] - 1s 5ms/step - loss: 0.0227 - accuracy: 0.9952\n",
            "Epoch 68/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.0226 - accuracy: 0.9952\n",
            "Epoch 69/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0225 - accuracy: 0.9952\n",
            "Epoch 70/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0225 - accuracy: 0.9952\n",
            "Epoch 71/100\n",
            "280/280 [==============================] - 1s 5ms/step - loss: 0.0226 - accuracy: 0.9952\n",
            "Epoch 72/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0225 - accuracy: 0.9952\n",
            "Epoch 73/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0224 - accuracy: 0.9952\n",
            "Epoch 74/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0224 - accuracy: 0.9952\n",
            "Epoch 75/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0223 - accuracy: 0.9952\n",
            "Epoch 76/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0223 - accuracy: 0.9952\n",
            "Epoch 77/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0221 - accuracy: 0.9952\n",
            "Epoch 78/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0223 - accuracy: 0.9952\n",
            "Epoch 79/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0222 - accuracy: 0.9952\n",
            "Epoch 80/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.0221 - accuracy: 0.9952\n",
            "Epoch 81/100\n",
            "280/280 [==============================] - 1s 5ms/step - loss: 0.0222 - accuracy: 0.9952\n",
            "Epoch 82/100\n",
            "280/280 [==============================] - 1s 5ms/step - loss: 0.0222 - accuracy: 0.9952\n",
            "Epoch 83/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0221 - accuracy: 0.9952\n",
            "Epoch 84/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0220 - accuracy: 0.9952\n",
            "Epoch 85/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0220 - accuracy: 0.9952\n",
            "Epoch 86/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0220 - accuracy: 0.9952\n",
            "Epoch 87/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0220 - accuracy: 0.9952\n",
            "Epoch 88/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0220 - accuracy: 0.9952\n",
            "Epoch 89/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0219 - accuracy: 0.9952\n",
            "Epoch 90/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0220 - accuracy: 0.9952\n",
            "Epoch 91/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0218 - accuracy: 0.9952\n",
            "Epoch 92/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0219 - accuracy: 0.9952\n",
            "Epoch 93/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0218 - accuracy: 0.9952\n",
            "Epoch 94/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0219 - accuracy: 0.9952\n",
            "Epoch 95/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0219 - accuracy: 0.9952\n",
            "Epoch 96/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.0218 - accuracy: 0.9952\n",
            "Epoch 97/100\n",
            "280/280 [==============================] - 1s 5ms/step - loss: 0.0217 - accuracy: 0.9952\n",
            "Epoch 98/100\n",
            "280/280 [==============================] - 1s 5ms/step - loss: 0.0218 - accuracy: 0.9952\n",
            "Epoch 99/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0217 - accuracy: 0.9952\n",
            "Epoch 100/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.0217 - accuracy: 0.9952\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a92b88f8970>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Evaluate GB, RF, XGB\n",
        "for model in [gb_model, rf_model, xgb_model]:\n",
        "    predictions = model.predict(X_test)\n",
        "    print(model.__class__.__name__)\n",
        "    print(classification_report(y_test, predictions))\n",
        "\n",
        "# Evaluate LSTM\n",
        "lstm_predictions = lstm_model.predict(X_test_reshaped)\n",
        "lstm_predictions = (lstm_predictions > 0.5).astype(int)  # Assuming binary classification\n",
        "print(\"LSTM Model\")\n",
        "print(classification_report(y_test, lstm_predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvqlmrJWFfPG",
        "outputId": "92b297b6-904b-4c58-e4fc-fe88dcb1d8cd"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GradientBoostingClassifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      2229\n",
            "           1       0.00      0.00      0.00        10\n",
            "\n",
            "    accuracy                           0.99      2239\n",
            "   macro avg       0.50      0.50      0.50      2239\n",
            "weighted avg       0.99      0.99      0.99      2239\n",
            "\n",
            "RandomForestClassifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      2229\n",
            "           1       0.00      0.00      0.00        10\n",
            "\n",
            "    accuracy                           1.00      2239\n",
            "   macro avg       0.50      0.50      0.50      2239\n",
            "weighted avg       0.99      1.00      0.99      2239\n",
            "\n",
            "XGBClassifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      2229\n",
            "           1       0.00      0.00      0.00        10\n",
            "\n",
            "    accuracy                           0.99      2239\n",
            "   macro avg       0.50      0.50      0.50      2239\n",
            "weighted avg       0.99      0.99      0.99      2239\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "70/70 [==============================] - 1s 4ms/step\n",
            "LSTM Model\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      2229\n",
            "           1       0.00      0.00      0.00        10\n",
            "\n",
            "    accuracy                           1.00      2239\n",
            "   macro avg       0.50      0.50      0.50      2239\n",
            "weighted avg       0.99      1.00      0.99      2239\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Resample the training data\n",
        "smote = SMOTE()\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "406zVJdRGBAy"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Boosting with resampled data\n",
        "gb_model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Random Forest with class weights\n",
        "rf_model = RandomForestClassifier(class_weight='balanced')\n",
        "rf_model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# XGBoost with scale_pos_weight parameter\n",
        "scale_pos_weight = sum(y_train == 0) / sum(y_train == 1)\n",
        "xgb_model = xgb.XGBClassifier(scale_pos_weight=scale_pos_weight)\n",
        "xgb_model.fit(X_train_resampled, y_train_resampled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "tsULrGzxGELU",
        "outputId": "fd42a23e-f147-4c7c-dd64-96602931afa3"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "              gamma=None, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
              "              num_parallel_tree=None, random_state=None, ...)"
            ],
            "text/html": [
              "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "              gamma=None, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
              "              num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "              gamma=None, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
              "              num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate XGBoost\n",
        "xgb_predictions = xgb_model.predict(X_test)\n",
        "print(\"XGBoost Classifier Evaluation\")\n",
        "print(classification_report(y_test, xgb_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOf29f5YGTyQ",
        "outputId": "d0176503-b32a-4d74-e3a8-069807f25563"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Classifier Evaluation\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.91      0.95      2229\n",
            "           1       0.03      0.70      0.07        10\n",
            "\n",
            "    accuracy                           0.91      2239\n",
            "   macro avg       0.52      0.81      0.51      2239\n",
            "weighted avg       0.99      0.91      0.95      2239\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "# Get probability estimates for class 1\n",
        "probabilities = gb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate precision-recall pairs for different threshold values\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_test, probabilities)\n",
        "\n",
        "# Select a threshold that balances precision and recall according to your needs\n",
        "# This is an illustrative example; choose a threshold based on your specific requirements\n",
        "selected_threshold = thresholds[np.argmax(precisions >= 0.05)]  # Example threshold criteria\n",
        "adjusted_predictions = (probabilities >= selected_threshold).astype(int)\n",
        "\n",
        "# Evaluate with the new threshold\n",
        "print(\"Adjusted Gradient Boosting Classifier Evaluation\")\n",
        "print(classification_report(y_test, adjusted_predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ocy0Rh7wGslB",
        "outputId": "0abc8071-e043-49ec-d762-85a6e6ebfa42"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusted Gradient Boosting Classifier Evaluation\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.97      0.98      2229\n",
            "           1       0.05      0.40      0.09        10\n",
            "\n",
            "    accuracy                           0.96      2239\n",
            "   macro avg       0.52      0.68      0.54      2239\n",
            "weighted avg       0.99      0.96      0.98      2239\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Data preparation as before\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, shuffle=False)\n",
        "X_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "# Building an advanced LSTM model\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(Bidirectional(LSTM(50, return_sequences=True, input_shape=(1, X_train.shape[1]))))\n",
        "lstm_model.add(Dropout(0.2))\n",
        "lstm_model.add(LSTM(50, return_sequences=False))\n",
        "lstm_model.add(Dropout(0.2))\n",
        "lstm_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "lstm_model.fit(X_train_reshaped, y_train, epochs=100, batch_size=32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Id6SeJCYHN3E",
        "outputId": "97ea0874-193d-4bbb-d921-fed8707b86d5"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "280/280 [==============================] - 14s 6ms/step - loss: 0.1778 - accuracy: 0.9662\n",
            "Epoch 2/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.0284 - accuracy: 0.9952\n",
            "Epoch 3/100\n",
            "280/280 [==============================] - 3s 9ms/step - loss: 0.0266 - accuracy: 0.9952\n",
            "Epoch 4/100\n",
            "280/280 [==============================] - 3s 9ms/step - loss: 0.0263 - accuracy: 0.9952\n",
            "Epoch 5/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.0263 - accuracy: 0.9952\n",
            "Epoch 6/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0264 - accuracy: 0.9952\n",
            "Epoch 7/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0265 - accuracy: 0.9952\n",
            "Epoch 8/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.0255 - accuracy: 0.9952\n",
            "Epoch 9/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.0254 - accuracy: 0.9952\n",
            "Epoch 10/100\n",
            "280/280 [==============================] - 3s 9ms/step - loss: 0.0257 - accuracy: 0.9952\n",
            "Epoch 11/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.0256 - accuracy: 0.9952\n",
            "Epoch 12/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0262 - accuracy: 0.9952\n",
            "Epoch 13/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.0259 - accuracy: 0.9952\n",
            "Epoch 14/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.0253 - accuracy: 0.9952\n",
            "Epoch 15/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.0246 - accuracy: 0.9952\n",
            "Epoch 16/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.0253 - accuracy: 0.9952\n",
            "Epoch 17/100\n",
            "280/280 [==============================] - 3s 10ms/step - loss: 0.0247 - accuracy: 0.9952\n",
            "Epoch 18/100\n",
            "280/280 [==============================] - 2s 9ms/step - loss: 0.0242 - accuracy: 0.9952\n",
            "Epoch 19/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.0249 - accuracy: 0.9952\n",
            "Epoch 20/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.0250 - accuracy: 0.9952\n",
            "Epoch 21/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.0250 - accuracy: 0.9952\n",
            "Epoch 22/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.0247 - accuracy: 0.9952\n",
            "Epoch 23/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0246 - accuracy: 0.9952\n",
            "Epoch 24/100\n",
            "280/280 [==============================] - 3s 10ms/step - loss: 0.0250 - accuracy: 0.9952\n",
            "Epoch 25/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0247 - accuracy: 0.9952\n",
            "Epoch 26/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.0239 - accuracy: 0.9952\n",
            "Epoch 27/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.0243 - accuracy: 0.9952\n",
            "Epoch 28/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.0242 - accuracy: 0.9952\n",
            "Epoch 29/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.0249 - accuracy: 0.9952\n",
            "Epoch 30/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.0236 - accuracy: 0.9952\n",
            "Epoch 31/100\n",
            "280/280 [==============================] - 3s 9ms/step - loss: 0.0240 - accuracy: 0.9952\n",
            "Epoch 32/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.0241 - accuracy: 0.9952\n",
            "Epoch 33/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0246 - accuracy: 0.9952\n",
            "Epoch 34/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0242 - accuracy: 0.9952\n",
            "Epoch 35/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0244 - accuracy: 0.9952\n",
            "Epoch 36/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0237 - accuracy: 0.9952\n",
            "Epoch 37/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0236 - accuracy: 0.9952\n",
            "Epoch 38/100\n",
            "280/280 [==============================] - 3s 10ms/step - loss: 0.0241 - accuracy: 0.9952\n",
            "Epoch 39/100\n",
            "280/280 [==============================] - 2s 9ms/step - loss: 0.0240 - accuracy: 0.9952\n",
            "Epoch 40/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0233 - accuracy: 0.9952\n",
            "Epoch 41/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.0239 - accuracy: 0.9952\n",
            "Epoch 42/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.0236 - accuracy: 0.9952\n",
            "Epoch 43/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.0240 - accuracy: 0.9952\n",
            "Epoch 44/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0232 - accuracy: 0.9952\n",
            "Epoch 45/100\n",
            "280/280 [==============================] - 3s 10ms/step - loss: 0.0240 - accuracy: 0.9952\n",
            "Epoch 46/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0243 - accuracy: 0.9951\n",
            "Epoch 47/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.0232 - accuracy: 0.9952\n",
            "Epoch 48/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0240 - accuracy: 0.9952\n",
            "Epoch 49/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0238 - accuracy: 0.9953\n",
            "Epoch 50/100\n",
            "280/280 [==============================] - 3s 10ms/step - loss: 0.0238 - accuracy: 0.9951\n",
            "Epoch 51/100\n",
            "280/280 [==============================] - 3s 11ms/step - loss: 0.0233 - accuracy: 0.9952\n",
            "Epoch 52/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.0234 - accuracy: 0.9952\n",
            "Epoch 53/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.0234 - accuracy: 0.9951\n",
            "Epoch 54/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0231 - accuracy: 0.9952\n",
            "Epoch 55/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0242 - accuracy: 0.9952\n",
            "Epoch 56/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0236 - accuracy: 0.9951\n",
            "Epoch 57/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.0235 - accuracy: 0.9952\n",
            "Epoch 58/100\n",
            "280/280 [==============================] - 3s 10ms/step - loss: 0.0234 - accuracy: 0.9952\n",
            "Epoch 59/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0237 - accuracy: 0.9952\n",
            "Epoch 60/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0228 - accuracy: 0.9951\n",
            "Epoch 61/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0235 - accuracy: 0.9952\n",
            "Epoch 62/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0233 - accuracy: 0.9950\n",
            "Epoch 63/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0237 - accuracy: 0.9952\n",
            "Epoch 64/100\n",
            "280/280 [==============================] - 3s 10ms/step - loss: 0.0232 - accuracy: 0.9951\n",
            "Epoch 65/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.0232 - accuracy: 0.9951\n",
            "Epoch 66/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0235 - accuracy: 0.9952\n",
            "Epoch 67/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0231 - accuracy: 0.9952\n",
            "Epoch 68/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0229 - accuracy: 0.9952\n",
            "Epoch 69/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0234 - accuracy: 0.9953\n",
            "Epoch 70/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0231 - accuracy: 0.9953\n",
            "Epoch 71/100\n",
            "280/280 [==============================] - 3s 10ms/step - loss: 0.0230 - accuracy: 0.9953\n",
            "Epoch 72/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.0235 - accuracy: 0.9951\n",
            "Epoch 73/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0229 - accuracy: 0.9952\n",
            "Epoch 74/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0227 - accuracy: 0.9952\n",
            "Epoch 75/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0234 - accuracy: 0.9950\n",
            "Epoch 76/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0225 - accuracy: 0.9952\n",
            "Epoch 77/100\n",
            "280/280 [==============================] - 3s 10ms/step - loss: 0.0235 - accuracy: 0.9952\n",
            "Epoch 78/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.0231 - accuracy: 0.9951\n",
            "Epoch 79/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0232 - accuracy: 0.9951\n",
            "Epoch 80/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0231 - accuracy: 0.9951\n",
            "Epoch 81/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.0228 - accuracy: 0.9952\n",
            "Epoch 82/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.0232 - accuracy: 0.9953\n",
            "Epoch 83/100\n",
            "280/280 [==============================] - 3s 10ms/step - loss: 0.0230 - accuracy: 0.9952\n",
            "Epoch 84/100\n",
            "280/280 [==============================] - 3s 11ms/step - loss: 0.0231 - accuracy: 0.9952\n",
            "Epoch 85/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.0231 - accuracy: 0.9952\n",
            "Epoch 86/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.0232 - accuracy: 0.9952\n",
            "Epoch 87/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.0232 - accuracy: 0.9953\n",
            "Epoch 88/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.0225 - accuracy: 0.9952\n",
            "Epoch 89/100\n",
            "280/280 [==============================] - 3s 12ms/step - loss: 0.0228 - accuracy: 0.9952\n",
            "Epoch 90/100\n",
            "280/280 [==============================] - 2s 9ms/step - loss: 0.0231 - accuracy: 0.9951\n",
            "Epoch 91/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0228 - accuracy: 0.9952\n",
            "Epoch 92/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0224 - accuracy: 0.9951\n",
            "Epoch 93/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.0222 - accuracy: 0.9952\n",
            "Epoch 94/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0226 - accuracy: 0.9952\n",
            "Epoch 95/100\n",
            "280/280 [==============================] - 3s 10ms/step - loss: 0.0229 - accuracy: 0.9951\n",
            "Epoch 96/100\n",
            "280/280 [==============================] - 3s 9ms/step - loss: 0.0225 - accuracy: 0.9952\n",
            "Epoch 97/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.0222 - accuracy: 0.9953\n",
            "Epoch 98/100\n",
            "280/280 [==============================] - 2s 9ms/step - loss: 0.0225 - accuracy: 0.9952\n",
            "Epoch 99/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0228 - accuracy: 0.9953\n",
            "Epoch 100/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.0224 - accuracy: 0.9953\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a92a9e17790>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Predicting with the LSTM model\n",
        "lstm_predictions = lstm_model.predict(X_test_reshaped)\n",
        "\n",
        "# Since we're doing binary classification,  might want to convert probabilities to binary predictions\n",
        "# can adjust the threshold based on y specific needs (default is 0.5)\n",
        "lstm_predictions_binary = (lstm_predictions > 0.5).astype(int)\n",
        "\n",
        "# Evaluating the model\n",
        "print(\"LSTM Model Performance:\")\n",
        "print(classification_report(y_test, lstm_predictions_binary))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, lstm_predictions_binary))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ll0nI0M9IKCo",
        "outputId": "26e0339b-7500-4d09-c05d-86332385b925"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "70/70 [==============================] - 1s 3ms/step\n",
            "LSTM Model Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      2229\n",
            "           1       0.00      0.00      0.00        10\n",
            "\n",
            "    accuracy                           1.00      2239\n",
            "   macro avg       0.50      0.50      0.50      2239\n",
            "weighted avg       0.99      1.00      0.99      2239\n",
            "\n",
            "Accuracy: 0.9955337204108977\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Calculate class weights\n",
        "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Use these class weights in model.fit\n",
        "lstm_model.fit(X_train_reshaped, y_train, epochs=100, batch_size=32, class_weight=class_weight_dict)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1MmsUmDIltb",
        "outputId": "739704f5-565d-4d99-8781-66f9f932a9df"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "280/280 [==============================] - 9s 7ms/step - loss: 0.6438 - accuracy: 0.9855\n",
            "Epoch 2/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.4291 - accuracy: 0.8464\n",
            "Epoch 3/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.3719 - accuracy: 0.7413\n",
            "Epoch 4/100\n",
            "280/280 [==============================] - 3s 10ms/step - loss: 0.3495 - accuracy: 0.7581\n",
            "Epoch 5/100\n",
            "280/280 [==============================] - 3s 9ms/step - loss: 0.3234 - accuracy: 0.7710\n",
            "Epoch 6/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3277 - accuracy: 0.7792\n",
            "Epoch 7/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3224 - accuracy: 0.7517\n",
            "Epoch 8/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3223 - accuracy: 0.7824\n",
            "Epoch 9/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3458 - accuracy: 0.7653\n",
            "Epoch 10/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.3238 - accuracy: 0.7658\n",
            "Epoch 11/100\n",
            "280/280 [==============================] - 3s 11ms/step - loss: 0.3200 - accuracy: 0.7775\n",
            "Epoch 12/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3252 - accuracy: 0.7758\n",
            "Epoch 13/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3274 - accuracy: 0.8124\n",
            "Epoch 14/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3160 - accuracy: 0.7800\n",
            "Epoch 15/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3277 - accuracy: 0.7925\n",
            "Epoch 16/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2966 - accuracy: 0.7988\n",
            "Epoch 17/100\n",
            "280/280 [==============================] - 3s 10ms/step - loss: 0.3419 - accuracy: 0.7998\n",
            "Epoch 18/100\n",
            "280/280 [==============================] - 2s 9ms/step - loss: 0.3229 - accuracy: 0.7857\n",
            "Epoch 19/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3036 - accuracy: 0.8094\n",
            "Epoch 20/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3050 - accuracy: 0.8038\n",
            "Epoch 21/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3050 - accuracy: 0.8015\n",
            "Epoch 22/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3053 - accuracy: 0.8003\n",
            "Epoch 23/100\n",
            "280/280 [==============================] - 3s 10ms/step - loss: 0.3156 - accuracy: 0.8082\n",
            "Epoch 24/100\n",
            "280/280 [==============================] - 3s 9ms/step - loss: 0.3116 - accuracy: 0.8072\n",
            "Epoch 25/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3008 - accuracy: 0.8015\n",
            "Epoch 26/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3030 - accuracy: 0.8128\n",
            "Epoch 27/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2968 - accuracy: 0.8032\n",
            "Epoch 28/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3036 - accuracy: 0.8023\n",
            "Epoch 29/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.3047 - accuracy: 0.7995\n",
            "Epoch 30/100\n",
            "280/280 [==============================] - 3s 10ms/step - loss: 0.3039 - accuracy: 0.8241\n",
            "Epoch 31/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.3015 - accuracy: 0.8203\n",
            "Epoch 32/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3201 - accuracy: 0.7987\n",
            "Epoch 33/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2975 - accuracy: 0.8134\n",
            "Epoch 34/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3047 - accuracy: 0.8124\n",
            "Epoch 35/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2975 - accuracy: 0.8109\n",
            "Epoch 36/100\n",
            "280/280 [==============================] - 3s 10ms/step - loss: 0.3036 - accuracy: 0.8145\n",
            "Epoch 37/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.2806 - accuracy: 0.8174\n",
            "Epoch 38/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2957 - accuracy: 0.8178\n",
            "Epoch 39/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2886 - accuracy: 0.8222\n",
            "Epoch 40/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3150 - accuracy: 0.8079\n",
            "Epoch 41/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2965 - accuracy: 0.8222\n",
            "Epoch 42/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.2843 - accuracy: 0.8154\n",
            "Epoch 43/100\n",
            "280/280 [==============================] - 3s 10ms/step - loss: 0.2922 - accuracy: 0.8156\n",
            "Epoch 44/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2869 - accuracy: 0.8221\n",
            "Epoch 45/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2883 - accuracy: 0.8150\n",
            "Epoch 46/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3004 - accuracy: 0.8300\n",
            "Epoch 47/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2809 - accuracy: 0.8165\n",
            "Epoch 48/100\n",
            "280/280 [==============================] - 3s 11ms/step - loss: 0.2932 - accuracy: 0.8379\n",
            "Epoch 49/100\n",
            "280/280 [==============================] - 3s 11ms/step - loss: 0.2756 - accuracy: 0.8185\n",
            "Epoch 50/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2763 - accuracy: 0.8345\n",
            "Epoch 51/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2908 - accuracy: 0.8340\n",
            "Epoch 52/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2833 - accuracy: 0.8170\n",
            "Epoch 53/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2703 - accuracy: 0.8376\n",
            "Epoch 54/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2723 - accuracy: 0.8374\n",
            "Epoch 55/100\n",
            "280/280 [==============================] - 3s 11ms/step - loss: 0.2734 - accuracy: 0.8245\n",
            "Epoch 56/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2851 - accuracy: 0.8351\n",
            "Epoch 57/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2743 - accuracy: 0.8235\n",
            "Epoch 58/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2914 - accuracy: 0.8154\n",
            "Epoch 59/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2850 - accuracy: 0.8217\n",
            "Epoch 60/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2731 - accuracy: 0.8221\n",
            "Epoch 61/100\n",
            "280/280 [==============================] - 3s 10ms/step - loss: 0.2716 - accuracy: 0.8348\n",
            "Epoch 62/100\n",
            "280/280 [==============================] - 3s 9ms/step - loss: 0.2899 - accuracy: 0.8119\n",
            "Epoch 63/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2700 - accuracy: 0.8423\n",
            "Epoch 64/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2596 - accuracy: 0.8392\n",
            "Epoch 65/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2888 - accuracy: 0.8350\n",
            "Epoch 66/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2966 - accuracy: 0.8358\n",
            "Epoch 67/100\n",
            "280/280 [==============================] - 2s 9ms/step - loss: 0.2793 - accuracy: 0.8240\n",
            "Epoch 68/100\n",
            "280/280 [==============================] - 3s 11ms/step - loss: 0.2743 - accuracy: 0.8245\n",
            "Epoch 69/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2821 - accuracy: 0.8320\n",
            "Epoch 70/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2700 - accuracy: 0.8309\n",
            "Epoch 71/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2568 - accuracy: 0.8386\n",
            "Epoch 72/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2774 - accuracy: 0.8287\n",
            "Epoch 73/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.2679 - accuracy: 0.8355\n",
            "Epoch 74/100\n",
            "280/280 [==============================] - 3s 12ms/step - loss: 0.2590 - accuracy: 0.8318\n",
            "Epoch 75/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.2841 - accuracy: 0.8347\n",
            "Epoch 76/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2665 - accuracy: 0.8280\n",
            "Epoch 77/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2779 - accuracy: 0.8332\n",
            "Epoch 78/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2594 - accuracy: 0.8364\n",
            "Epoch 79/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2580 - accuracy: 0.8439\n",
            "Epoch 80/100\n",
            "280/280 [==============================] - 3s 11ms/step - loss: 0.2717 - accuracy: 0.8300\n",
            "Epoch 81/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.2690 - accuracy: 0.8216\n",
            "Epoch 82/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2590 - accuracy: 0.8298\n",
            "Epoch 83/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2636 - accuracy: 0.8338\n",
            "Epoch 84/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2787 - accuracy: 0.8261\n",
            "Epoch 85/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2667 - accuracy: 0.8375\n",
            "Epoch 86/100\n",
            "280/280 [==============================] - 3s 10ms/step - loss: 0.2473 - accuracy: 0.8418\n",
            "Epoch 87/100\n",
            "280/280 [==============================] - 3s 9ms/step - loss: 0.2447 - accuracy: 0.8565\n",
            "Epoch 88/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2577 - accuracy: 0.8423\n",
            "Epoch 89/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2901 - accuracy: 0.8173\n",
            "Epoch 90/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2622 - accuracy: 0.8118\n",
            "Epoch 91/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2620 - accuracy: 0.8329\n",
            "Epoch 92/100\n",
            "280/280 [==============================] - 3s 10ms/step - loss: 0.2741 - accuracy: 0.8221\n",
            "Epoch 93/100\n",
            "280/280 [==============================] - 3s 9ms/step - loss: 0.2613 - accuracy: 0.8279\n",
            "Epoch 94/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2843 - accuracy: 0.8288\n",
            "Epoch 95/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2670 - accuracy: 0.8263\n",
            "Epoch 96/100\n",
            "280/280 [==============================] - 2s 9ms/step - loss: 0.2614 - accuracy: 0.8291\n",
            "Epoch 97/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2631 - accuracy: 0.8272\n",
            "Epoch 98/100\n",
            "280/280 [==============================] - 3s 9ms/step - loss: 0.2491 - accuracy: 0.8394\n",
            "Epoch 99/100\n",
            "280/280 [==============================] - 3s 10ms/step - loss: 0.2544 - accuracy: 0.8425\n",
            "Epoch 100/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2586 - accuracy: 0.8291\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a92abe320e0>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Predicting with the LSTM model\n",
        "lstm_predictions = lstm_model.predict(X_test_reshaped)\n",
        "lstm_predictions_binary = (lstm_predictions > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
        "\n",
        "# Evaluating the model\n",
        "print(\"LSTM Model Performance with Class Weights:\")\n",
        "print(classification_report(y_test, lstm_predictions_binary))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, lstm_predictions_binary))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTbHBa-FJ5JX",
        "outputId": "da4aaf05-2330-480e-eea9-183a8ca1b91a"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "70/70 [==============================] - 1s 7ms/step\n",
            "LSTM Model Performance with Class Weights:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.85      0.92      2229\n",
            "           1       0.02      0.80      0.05        10\n",
            "\n",
            "    accuracy                           0.85      2239\n",
            "   macro avg       0.51      0.83      0.48      2239\n",
            "weighted avg       0.99      0.85      0.92      2239\n",
            "\n",
            "Accuracy: 0.8530594015185351\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "\n",
        "# Enhanced LSTM Model\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(Bidirectional(LSTM(50, return_sequences=True, input_shape=(1, X_train.shape[1]))))\n",
        "lstm_model.add(Dropout(0.3))\n",
        "lstm_model.add(LSTM(100, return_sequences=False))\n",
        "lstm_model.add(Dropout(0.3))\n",
        "lstm_model.add(Dense(50, activation='relu'))\n",
        "lstm_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "fE_b4SMvKe6G"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming class_weight_dict is already calculated\n",
        "lstm_model.fit(X_train_reshaped, y_train, epochs=100, batch_size=32, class_weight=class_weight_dict)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1iP6uPiKiRv",
        "outputId": "3dc2d1c0-a3e3-4cca-fb6c-9e6d66390725"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "280/280 [==============================] - 22s 14ms/step - loss: 0.6265 - accuracy: 0.8091\n",
            "Epoch 2/100\n",
            "280/280 [==============================] - 3s 9ms/step - loss: 0.4652 - accuracy: 0.7121\n",
            "Epoch 3/100\n",
            "280/280 [==============================] - 3s 10ms/step - loss: 0.4424 - accuracy: 0.7179\n",
            "Epoch 4/100\n",
            "280/280 [==============================] - 3s 11ms/step - loss: 0.4409 - accuracy: 0.6830\n",
            "Epoch 5/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.4355 - accuracy: 0.6703\n",
            "Epoch 6/100\n",
            "280/280 [==============================] - 3s 10ms/step - loss: 0.4144 - accuracy: 0.7247\n",
            "Epoch 7/100\n",
            "280/280 [==============================] - 3s 9ms/step - loss: 0.3934 - accuracy: 0.7146\n",
            "Epoch 8/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3928 - accuracy: 0.7215\n",
            "Epoch 9/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3784 - accuracy: 0.7299\n",
            "Epoch 10/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.4400 - accuracy: 0.7358\n",
            "Epoch 11/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3750 - accuracy: 0.7485\n",
            "Epoch 12/100\n",
            "280/280 [==============================] - 3s 10ms/step - loss: 0.3788 - accuracy: 0.7572\n",
            "Epoch 13/100\n",
            "280/280 [==============================] - 3s 10ms/step - loss: 0.3759 - accuracy: 0.7476\n",
            "Epoch 14/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3995 - accuracy: 0.7403\n",
            "Epoch 15/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.4077 - accuracy: 0.7185\n",
            "Epoch 16/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3814 - accuracy: 0.7320\n",
            "Epoch 17/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.3693 - accuracy: 0.7517\n",
            "Epoch 18/100\n",
            "280/280 [==============================] - 3s 9ms/step - loss: 0.3559 - accuracy: 0.7453\n",
            "Epoch 19/100\n",
            "280/280 [==============================] - 3s 10ms/step - loss: 0.3766 - accuracy: 0.7555\n",
            "Epoch 20/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3525 - accuracy: 0.7260\n",
            "Epoch 21/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3730 - accuracy: 0.7500\n",
            "Epoch 22/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.4035 - accuracy: 0.7428\n",
            "Epoch 23/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3624 - accuracy: 0.7307\n",
            "Epoch 24/100\n",
            "280/280 [==============================] - 2s 9ms/step - loss: 0.3415 - accuracy: 0.7549\n",
            "Epoch 25/100\n",
            "280/280 [==============================] - 3s 10ms/step - loss: 0.3752 - accuracy: 0.7574\n",
            "Epoch 26/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.3567 - accuracy: 0.7647\n",
            "Epoch 27/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3768 - accuracy: 0.7441\n",
            "Epoch 28/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3353 - accuracy: 0.7515\n",
            "Epoch 29/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3922 - accuracy: 0.7401\n",
            "Epoch 30/100\n",
            "280/280 [==============================] - 2s 9ms/step - loss: 0.3385 - accuracy: 0.7546\n",
            "Epoch 31/100\n",
            "280/280 [==============================] - 3s 11ms/step - loss: 0.3343 - accuracy: 0.7802\n",
            "Epoch 32/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3647 - accuracy: 0.7510\n",
            "Epoch 33/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3673 - accuracy: 0.7729\n",
            "Epoch 34/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3305 - accuracy: 0.7806\n",
            "Epoch 35/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3323 - accuracy: 0.7817\n",
            "Epoch 36/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.3274 - accuracy: 0.7742\n",
            "Epoch 37/100\n",
            "280/280 [==============================] - 3s 12ms/step - loss: 0.3912 - accuracy: 0.7772\n",
            "Epoch 38/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3608 - accuracy: 0.7508\n",
            "Epoch 39/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3346 - accuracy: 0.7587\n",
            "Epoch 40/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3678 - accuracy: 0.7661\n",
            "Epoch 41/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.3317 - accuracy: 0.7591\n",
            "Epoch 42/100\n",
            "280/280 [==============================] - 2s 9ms/step - loss: 0.3101 - accuracy: 0.7855\n",
            "Epoch 43/100\n",
            "280/280 [==============================] - 3s 11ms/step - loss: 0.3431 - accuracy: 0.7869\n",
            "Epoch 44/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3291 - accuracy: 0.7747\n",
            "Epoch 45/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3184 - accuracy: 0.7896\n",
            "Epoch 46/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3245 - accuracy: 0.7866\n",
            "Epoch 47/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3418 - accuracy: 0.7649\n",
            "Epoch 48/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3283 - accuracy: 0.7785\n",
            "Epoch 49/100\n",
            "280/280 [==============================] - 3s 12ms/step - loss: 0.3533 - accuracy: 0.7853\n",
            "Epoch 50/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.3371 - accuracy: 0.7558\n",
            "Epoch 51/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3144 - accuracy: 0.7802\n",
            "Epoch 52/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.3054 - accuracy: 0.8016\n",
            "Epoch 53/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.3208 - accuracy: 0.7983\n",
            "Epoch 54/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.3179 - accuracy: 0.7864\n",
            "Epoch 55/100\n",
            "280/280 [==============================] - 3s 12ms/step - loss: 0.3244 - accuracy: 0.7986\n",
            "Epoch 56/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.3166 - accuracy: 0.7881\n",
            "Epoch 57/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.3524 - accuracy: 0.7952\n",
            "Epoch 58/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.3011 - accuracy: 0.8125\n",
            "Epoch 59/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.3200 - accuracy: 0.7967\n",
            "Epoch 60/100\n",
            "280/280 [==============================] - 3s 10ms/step - loss: 0.3542 - accuracy: 0.7934\n",
            "Epoch 61/100\n",
            "280/280 [==============================] - 3s 11ms/step - loss: 0.3138 - accuracy: 0.7880\n",
            "Epoch 62/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.3198 - accuracy: 0.8071\n",
            "Epoch 63/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3127 - accuracy: 0.7831\n",
            "Epoch 64/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3029 - accuracy: 0.8023\n",
            "Epoch 65/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.3222 - accuracy: 0.8146\n",
            "Epoch 66/100\n",
            "280/280 [==============================] - 3s 11ms/step - loss: 0.3003 - accuracy: 0.7987\n",
            "Epoch 67/100\n",
            "280/280 [==============================] - 3s 9ms/step - loss: 0.3002 - accuracy: 0.8137\n",
            "Epoch 68/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.3279 - accuracy: 0.8210\n",
            "Epoch 69/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.2960 - accuracy: 0.8098\n",
            "Epoch 70/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2889 - accuracy: 0.8179\n",
            "Epoch 71/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3164 - accuracy: 0.7979\n",
            "Epoch 72/100\n",
            "280/280 [==============================] - 3s 12ms/step - loss: 0.2921 - accuracy: 0.8192\n",
            "Epoch 73/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.2792 - accuracy: 0.8237\n",
            "Epoch 74/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.2983 - accuracy: 0.8245\n",
            "Epoch 75/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3005 - accuracy: 0.8105\n",
            "Epoch 76/100\n",
            "280/280 [==============================] - 3s 10ms/step - loss: 0.3358 - accuracy: 0.8032\n",
            "Epoch 77/100\n",
            "280/280 [==============================] - 3s 11ms/step - loss: 0.3142 - accuracy: 0.8027\n",
            "Epoch 78/100\n",
            "280/280 [==============================] - 3s 10ms/step - loss: 0.3003 - accuracy: 0.8168\n",
            "Epoch 79/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.3003 - accuracy: 0.8106\n",
            "Epoch 80/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.3032 - accuracy: 0.8135\n",
            "Epoch 81/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.2975 - accuracy: 0.8328\n",
            "Epoch 82/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2980 - accuracy: 0.8216\n",
            "Epoch 83/100\n",
            "280/280 [==============================] - 3s 11ms/step - loss: 0.2974 - accuracy: 0.8117\n",
            "Epoch 84/100\n",
            "280/280 [==============================] - 3s 10ms/step - loss: 0.3086 - accuracy: 0.8074\n",
            "Epoch 85/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.2951 - accuracy: 0.8071\n",
            "Epoch 86/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.2954 - accuracy: 0.8184\n",
            "Epoch 87/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2805 - accuracy: 0.8351\n",
            "Epoch 88/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3207 - accuracy: 0.8127\n",
            "Epoch 89/100\n",
            "280/280 [==============================] - 3s 11ms/step - loss: 0.3735 - accuracy: 0.7922\n",
            "Epoch 90/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.2921 - accuracy: 0.8053\n",
            "Epoch 91/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.2972 - accuracy: 0.8284\n",
            "Epoch 92/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.3287 - accuracy: 0.8148\n",
            "Epoch 93/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.2870 - accuracy: 0.8233\n",
            "Epoch 94/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.2930 - accuracy: 0.8339\n",
            "Epoch 95/100\n",
            "280/280 [==============================] - 3s 12ms/step - loss: 0.2747 - accuracy: 0.8259\n",
            "Epoch 96/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.2751 - accuracy: 0.8342\n",
            "Epoch 97/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.2975 - accuracy: 0.8192\n",
            "Epoch 98/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3298 - accuracy: 0.8162\n",
            "Epoch 99/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.3132 - accuracy: 0.8213\n",
            "Epoch 100/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.2789 - accuracy: 0.8179\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a92b8c285e0>"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Predicting probabilities\n",
        "lstm_probabilities = lstm_model.predict(X_test_reshaped)\n",
        "\n",
        "# Adjusting the decision threshold\n",
        "threshold = 0.5\n",
        "lstm_predictions = (lstm_probabilities > threshold).astype(int)\n",
        "\n",
        "# Evaluating the model\n",
        "print(\"Adjusted LSTM Model Performance:\")\n",
        "print(classification_report(y_test, lstm_predictions))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, lstm_predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l30uIXc9KkCw",
        "outputId": "3761c2b6-7349-49e0-db31-b77a38c0152f"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "70/70 [==============================] - 1s 3ms/step\n",
            "Adjusted LSTM Model Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.78      0.88      2229\n",
            "           1       0.02      1.00      0.04        10\n",
            "\n",
            "    accuracy                           0.79      2239\n",
            "   macro avg       0.51      0.89      0.46      2239\n",
            "weighted avg       1.00      0.79      0.88      2239\n",
            "\n",
            "Accuracy: 0.7856185797230907\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simplified LSTM Model\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(LSTM(50, input_shape=(1, X_train.shape[1])))\n",
        "lstm_model.add(Dropout(0.2))\n",
        "lstm_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "a2KWAbvDLyNd"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retraining the model with class weights\n",
        "lstm_model.fit(X_train_reshaped, y_train, epochs=100, batch_size=32, class_weight=class_weight_dict)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ylwyhVVMYJE",
        "outputId": "65b77a17-80f1-4f9e-d3b1-862a1bd33615"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "280/280 [==============================] - 6s 6ms/step - loss: 0.6525 - accuracy: 0.5914\n",
            "Epoch 2/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.5630 - accuracy: 0.6326\n",
            "Epoch 3/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.5042 - accuracy: 0.6458\n",
            "Epoch 4/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.4675 - accuracy: 0.7031\n",
            "Epoch 5/100\n",
            "280/280 [==============================] - 1s 5ms/step - loss: 0.4482 - accuracy: 0.7221\n",
            "Epoch 6/100\n",
            "280/280 [==============================] - 1s 5ms/step - loss: 0.4390 - accuracy: 0.7308\n",
            "Epoch 7/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.4286 - accuracy: 0.7488\n",
            "Epoch 8/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.4213 - accuracy: 0.7440\n",
            "Epoch 9/100\n",
            "280/280 [==============================] - 1s 5ms/step - loss: 0.4137 - accuracy: 0.7533\n",
            "Epoch 10/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.4064 - accuracy: 0.7479\n",
            "Epoch 11/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.4072 - accuracy: 0.7669\n",
            "Epoch 12/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.4063 - accuracy: 0.7538\n",
            "Epoch 13/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.4024 - accuracy: 0.7591\n",
            "Epoch 14/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3980 - accuracy: 0.7647\n",
            "Epoch 15/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3869 - accuracy: 0.7538\n",
            "Epoch 16/100\n",
            "280/280 [==============================] - 1s 5ms/step - loss: 0.3936 - accuracy: 0.7717\n",
            "Epoch 17/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3883 - accuracy: 0.7777\n",
            "Epoch 18/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3847 - accuracy: 0.7644\n",
            "Epoch 19/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3816 - accuracy: 0.7677\n",
            "Epoch 20/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.3807 - accuracy: 0.7710\n",
            "Epoch 21/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.3848 - accuracy: 0.7785\n",
            "Epoch 22/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3745 - accuracy: 0.7705\n",
            "Epoch 23/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3833 - accuracy: 0.7706\n",
            "Epoch 24/100\n",
            "280/280 [==============================] - 2s 8ms/step - loss: 0.3708 - accuracy: 0.7810\n",
            "Epoch 25/100\n",
            "280/280 [==============================] - 1s 5ms/step - loss: 0.3820 - accuracy: 0.7728\n",
            "Epoch 26/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3683 - accuracy: 0.7861\n",
            "Epoch 27/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3646 - accuracy: 0.7702\n",
            "Epoch 28/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.3652 - accuracy: 0.7687\n",
            "Epoch 29/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.3712 - accuracy: 0.7830\n",
            "Epoch 30/100\n",
            "280/280 [==============================] - 1s 5ms/step - loss: 0.3718 - accuracy: 0.7879\n",
            "Epoch 31/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3632 - accuracy: 0.7699\n",
            "Epoch 32/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3695 - accuracy: 0.7887\n",
            "Epoch 33/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3647 - accuracy: 0.7790\n",
            "Epoch 34/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3586 - accuracy: 0.7794\n",
            "Epoch 35/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3538 - accuracy: 0.7834\n",
            "Epoch 36/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3446 - accuracy: 0.7895\n",
            "Epoch 37/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3617 - accuracy: 0.7746\n",
            "Epoch 38/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3541 - accuracy: 0.7905\n",
            "Epoch 39/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.3581 - accuracy: 0.7921\n",
            "Epoch 40/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.3478 - accuracy: 0.7833\n",
            "Epoch 41/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3424 - accuracy: 0.7867\n",
            "Epoch 42/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3620 - accuracy: 0.7911\n",
            "Epoch 43/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3442 - accuracy: 0.7820\n",
            "Epoch 44/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3532 - accuracy: 0.7807\n",
            "Epoch 45/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3465 - accuracy: 0.7959\n",
            "Epoch 46/100\n",
            "280/280 [==============================] - 1s 5ms/step - loss: 0.3436 - accuracy: 0.7878\n",
            "Epoch 47/100\n",
            "280/280 [==============================] - 1s 5ms/step - loss: 0.3373 - accuracy: 0.7936\n",
            "Epoch 48/100\n",
            "280/280 [==============================] - 1s 5ms/step - loss: 0.3354 - accuracy: 0.8084\n",
            "Epoch 49/100\n",
            "280/280 [==============================] - 2s 7ms/step - loss: 0.3434 - accuracy: 0.7956\n",
            "Epoch 50/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.3335 - accuracy: 0.8001\n",
            "Epoch 51/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3498 - accuracy: 0.7918\n",
            "Epoch 52/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3404 - accuracy: 0.7879\n",
            "Epoch 53/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3474 - accuracy: 0.7935\n",
            "Epoch 54/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3510 - accuracy: 0.8001\n",
            "Epoch 55/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3352 - accuracy: 0.7889\n",
            "Epoch 56/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3310 - accuracy: 0.8090\n",
            "Epoch 57/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3523 - accuracy: 0.7887\n",
            "Epoch 58/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3390 - accuracy: 0.7957\n",
            "Epoch 59/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3317 - accuracy: 0.7968\n",
            "Epoch 60/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.3375 - accuracy: 0.7835\n",
            "Epoch 61/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.3448 - accuracy: 0.7922\n",
            "Epoch 62/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3335 - accuracy: 0.7941\n",
            "Epoch 63/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3508 - accuracy: 0.7934\n",
            "Epoch 64/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3421 - accuracy: 0.7952\n",
            "Epoch 65/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3284 - accuracy: 0.7896\n",
            "Epoch 66/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3508 - accuracy: 0.7947\n",
            "Epoch 67/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3343 - accuracy: 0.7885\n",
            "Epoch 68/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3313 - accuracy: 0.8045\n",
            "Epoch 69/100\n",
            "280/280 [==============================] - 2s 5ms/step - loss: 0.3375 - accuracy: 0.7957\n",
            "Epoch 70/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.3442 - accuracy: 0.7888\n",
            "Epoch 71/100\n",
            "280/280 [==============================] - 2s 5ms/step - loss: 0.3301 - accuracy: 0.7824\n",
            "Epoch 72/100\n",
            "280/280 [==============================] - 1s 5ms/step - loss: 0.3357 - accuracy: 0.7971\n",
            "Epoch 73/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3405 - accuracy: 0.8026\n",
            "Epoch 74/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3375 - accuracy: 0.7914\n",
            "Epoch 75/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3226 - accuracy: 0.7958\n",
            "Epoch 76/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3403 - accuracy: 0.7853\n",
            "Epoch 77/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3332 - accuracy: 0.8012\n",
            "Epoch 78/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3301 - accuracy: 0.7919\n",
            "Epoch 79/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3297 - accuracy: 0.8038\n",
            "Epoch 80/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3326 - accuracy: 0.7898\n",
            "Epoch 81/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3183 - accuracy: 0.7952\n",
            "Epoch 82/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.3354 - accuracy: 0.8073\n",
            "Epoch 83/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.3316 - accuracy: 0.7968\n",
            "Epoch 84/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3292 - accuracy: 0.7956\n",
            "Epoch 85/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3249 - accuracy: 0.7993\n",
            "Epoch 86/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3401 - accuracy: 0.7952\n",
            "Epoch 87/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 0.3376 - accuracy: 0.7992\n",
            "Epoch 88/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3348 - accuracy: 0.7860\n",
            "Epoch 89/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3424 - accuracy: 0.7924\n",
            "Epoch 90/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3316 - accuracy: 0.7962\n",
            "Epoch 91/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3264 - accuracy: 0.7957\n",
            "Epoch 92/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3365 - accuracy: 0.7907\n",
            "Epoch 93/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3244 - accuracy: 0.7916\n",
            "Epoch 94/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.3413 - accuracy: 0.7957\n",
            "Epoch 95/100\n",
            "280/280 [==============================] - 2s 6ms/step - loss: 0.3202 - accuracy: 0.7971\n",
            "Epoch 96/100\n",
            "280/280 [==============================] - 1s 5ms/step - loss: 0.3351 - accuracy: 0.7908\n",
            "Epoch 97/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3278 - accuracy: 0.8006\n",
            "Epoch 98/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3191 - accuracy: 0.8012\n",
            "Epoch 99/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3235 - accuracy: 0.7901\n",
            "Epoch 100/100\n",
            "280/280 [==============================] - 1s 4ms/step - loss: 0.3322 - accuracy: 0.7964\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a92a34c7b20>"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting probabilities\n",
        "lstm_probabilities = lstm_model.predict(X_test_reshaped)\n",
        "\n",
        "# Experiment with different thresholds\n",
        "thresholds = [0.4, 0.5, 0.6]  # Example thresholds, adjust as needed\n",
        "\n",
        "for threshold in thresholds:\n",
        "    lstm_predictions = (lstm_probabilities > threshold).astype(int)\n",
        "    print(f\"LSTM Model Performance at Threshold {threshold}:\")\n",
        "    print(classification_report(y_test, lstm_predictions))\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, lstm_predictions))\n",
        "    print(\"-------------------------------------------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHmU0q2FMgmz",
        "outputId": "118a70d2-6046-4114-a5fc-232074e3f471"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "70/70 [==============================] - 1s 2ms/step\n",
            "LSTM Model Performance at Threshold 0.4:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.73      0.84      2229\n",
            "           1       0.01      0.90      0.03        10\n",
            "\n",
            "    accuracy                           0.73      2239\n",
            "   macro avg       0.51      0.82      0.44      2239\n",
            "weighted avg       0.99      0.73      0.84      2239\n",
            "\n",
            "Accuracy: 0.7324698526127735\n",
            "-------------------------------------------\n",
            "LSTM Model Performance at Threshold 0.5:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.76      0.87      2229\n",
            "           1       0.02      0.90      0.03        10\n",
            "\n",
            "    accuracy                           0.76      2239\n",
            "   macro avg       0.51      0.83      0.45      2239\n",
            "weighted avg       1.00      0.76      0.86      2239\n",
            "\n",
            "Accuracy: 0.7641804376953998\n",
            "-------------------------------------------\n",
            "LSTM Model Performance at Threshold 0.6:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.80      0.89      2229\n",
            "           1       0.02      0.90      0.04        10\n",
            "\n",
            "    accuracy                           0.80      2239\n",
            "   macro avg       0.51      0.85      0.46      2239\n",
            "weighted avg       1.00      0.80      0.88      2239\n",
            "\n",
            "Accuracy: 0.7963376507369362\n",
            "-------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}